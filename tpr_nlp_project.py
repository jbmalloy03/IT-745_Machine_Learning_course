# -*- coding: utf-8 -*-
"""TPR_NLP_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jIjGBq6_eAvtX2lNquhRrYts20Lw3jc4
"""

dashboard_description = """AI-Driven Third-Party Risk Assessment Dashboard (Enhanced)
----------------------------------------------------------
Interactive Streamlit dashboard that performs third-party vendor
risk assessment using questionnaires, NLP similarity scoring"""

import streamlit as st
import pandas as pd
import json
import numpy as np
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import docx
import PyPDF2

# =========================================================
# Questionnaire Configuration
# =========================================================
questions = {
    "q1": {"text": "Does the vendor have a formal cybersecurity policy?", "weight": 10},
    "q2": {"text": "Is data encrypted in transit and at rest?", "weight": 10},
    "q3": {"text": "Does the vendor perform regular vulnerability assessments?", "weight": 10},
    "q4": {"text": "Is multi-factor authentication (MFA) implemented?", "weight": 8},
    "q5": {"text": "Does the vendor comply with relevant regulations (e.g., GDPR, HIPAA)?", "weight": 10},
    "q6": {"text": "Does the vendor store or process sensitive data?", "weight": 7},
    "q7": {"text": "Does the vendor subcontract any critical services?", "weight": 8},
    "q8": {"text": "Is the vendorâ€™s incident response plan tested annually?", "weight": 10},
    "q9": {"text": "Has the vendor experienced any recent data breaches?", "weight": 12},
    "q10": {"text": "Does the vendor provide employee cybersecurity training?", "weight": 5}
}

# =========================================================
# Control Corpus for NLP Similarity
# =========================================================
CONTROL_CORPUS = {
    "Access Control": "access control authentication authorization least privilege",
    "Incident Response": "incident response detection containment recovery testing",
    "Data Protection": "data protection encryption privacy retention classification",
    "Business Continuity": "business continuity disaster recovery resilience testing",
    "Vendor Governance": "vendor risk governance oversight compliance monitoring"
}

# =========================================================
# Helper Functions
# =========================================================
def extract_policy_text(uploaded_file):
    text = ""
    if uploaded_file.type == "application/pdf":
        reader = PyPDF2.PdfReader(uploaded_file)
        for page in reader.pages:
            text += page.extract_text() or ""
    elif uploaded_file.type == (
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    ):
        doc = docx.Document(uploaded_file)
        for para in doc.paragraphs:
            text += para.text + " "
    return text.lower()


def calculate_nlp_similarity(policy_text):
    vectorizer = TfidfVectorizer(stop_words="english")
    corpus = [policy_text] + list(CONTROL_CORPUS.values())
    tfidf = vectorizer.fit_transform(corpus)
    similarities = cosine_similarity(tfidf[0:1], tfidf[1:])
    return similarities.flatten()


def calculate_risk_score(responses, criticality):
    total_weight = sum(q["weight"] for q in questions.values())
    weighted_score = 0

    for key, response in responses.items():
        weight = questions[key]["weight"]
        if response == "Yes":
            weighted_score += weight
        elif response == "Partial":
            weighted_score += weight * 0.5

    score = (weighted_score / total_weight) * 100

    if criticality == "High":
        score *= 0.9
    elif criticality == "Low":
        score *= 1.1

    return min(100, max(0, score))


def classify_risk(score):
    if score < 50:
        return "High Risk"
    elif score < 75:
        return "Medium Risk"
    else:
        return "Low Risk"


def generate_nlp_insights(similarity_scores):
    insights = []
    for idx, (domain, score) in enumerate(
        zip(CONTROL_CORPUS.keys(), similarity_scores)
    ):
        if score >= 0.6:
            insights.append(f"{domain}: Strong alignment observed.")
        elif score >= 0.4:
            insights.append(f"{domain}: Partial alignment; improvements recommended.")
        else:
            insights.append(f"{domain}: Weak alignment; control gaps likely present.")
    return insights


def generate_recommendations(risk_class, similarity_scores):
    recs = []

    if risk_class == "High Risk":
        recs.append("Initiate immediate remediation planning and enhanced oversight.")
    elif risk_class == "Medium Risk":
        recs.append("Address identified control gaps and reassess within 6â€“12 months.")
    else:
        recs.append("Maintain standard monitoring and periodic reassessment.")

    if np.mean(similarity_scores) < 0.45:
        recs.append("Policy documentation shows low alignment with baseline controls.")

    return recs

# =========================================================
# Streamlit UI
# =========================================================
st.set_page_config(
    page_title="AI-Driven Third-Party Risk Assessment",
    layout="centered"
)

st.title("ðŸ§  AI-Driven Third-Party Risk Assessment Dashboard")
st.write("Assess vendor cybersecurity readiness using questionnaires and explainable NLP analysis.")

# Vendor Information
st.header("Vendor Information")
vendor = st.text_input("Vendor Name", "SampleVendorCo")
industry = st.text_input("Vendor Industry", "Finance")
criticality = st.selectbox("Vendor Criticality", ["High", "Medium", "Low"])

# Questionnaire
st.header("Risk Assessment Questionnaire")
responses = {}
for key, q in questions.items():
    responses[key] = st.radio(q["text"], ["Yes", "No", "Partial"], horizontal=True)

# Policy Upload
st.header("Upload Vendor Policy Documentation")
uploaded_file = st.file_uploader("Upload PDF or DOCX", type=["pdf", "docx"])

# Run Assessment
if st.button("Run AI-Driven Assessment"):

    score = calculate_risk_score(responses, criticality)
    risk_class = classify_risk(score)
    timestamp = datetime.utcnow().isoformat() + "Z"

    similarity_scores = None
    insights = []

    if uploaded_file:
        policy_text = extract_policy_text(uploaded_file)
        similarity_scores = calculate_nlp_similarity(policy_text)
        insights = generate_nlp_insights(similarity_scores)

    recommendations = generate_recommendations(
        risk_class,
        similarity_scores if similarity_scores is not None else []
    )

    # Results
    st.subheader("Assessment Results")
    st.metric("Risk Score", f"{round(score,1)} / 100")
    st.metric("Risk Classification", risk_class)
    st.progress(int(score))

    if similarity_scores is not None:
        st.write("### ðŸ” NLP Policy Alignment Insights")
        for i in insights:
            st.markdown(f"- {i}")

    st.write("### ðŸ§© Recommendations")
    for r in recommendations:
        st.markdown(f"- {r}")

    # Export
    result = {
        "vendor": vendor,
        "industry": industry,
        "criticality": criticality,
        "risk_score": round(score, 1),
        "risk_classification": risk_class,
        "nlp_similarity_scores": dict(zip(CONTROL_CORPUS.keys(), similarity_scores))
        if similarity_scores is not None else None,
        "recommendations": recommendations,
        "responses": responses,
        "timestamp": timestamp
    }

    st.download_button(
        "Download JSON Report",
        json.dumps(result, indent=4),
        f"AI_TPRM_Report_{vendor}.json",
        "application/json"
    )

    df = pd.DataFrame([{
        "vendor": vendor,
        "industry": industry,
        "criticality": criticality,
        "risk_score": score,
        "risk_classification": risk_class,
        "timestamp": timestamp
    }])

    st.download_button(
        "Download CSV Summary",
        df.to_csv(index=False),
        f"AI_TPRM_Report_{vendor}.csv",
        "text/csv"
    )

st.markdown("---")
st.caption("AI-Driven Cybersecurity Research on Third-Party Risk Management Â© 2026")